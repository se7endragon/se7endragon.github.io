---
layout: post
title: "Data Mining Lecture"
date: 2019-04-11
image: '/assets/img/'
description: 'Lecture 6'
categories:
- Lecture Notes
- Data Mining
---

# 유사도, 이웃, 군집

어떤 측면에서 비슷하다면 다른 특징 또한 같을 경우도 종종 있다는 가정 하에 유사도를 이용해 객체를 그룹화하거나 유사도를 판단하는 올바른 기준을 찾는 작업.

- 유사도 : 기업에서 잠재 고객을 찾기 위해 우수 고객과 유사한 다른 고객 발굴
- 이웃 : 아마존이나 넷플릭스 같은 기업에서 기존에 구입한 제품과 비슷한 제품을 추천하거나 비슷한 사람들이 구매한 제품 추천 기능 제공.
- 군집 : 우리 회사 고객 중 비슷한 고객들이 있는지, 이 군집에 어떤 공통점이 있는지 알 수 있도록 비슷한 군집으로 묶어서 분석

유사도가 높으면 이웃, 이웃이 뭉치면 군집.

### 이웃점수 매기기 (Combining Function)
객체의 최근접 이웃으로부터 객체의 타깃값을 예측하는데 사용되는 공식

- 다수결 투표 분류
  - 최근접 이웃의 최댓값 반환
  - c(x) = argmax score(c,neighbors(x))

- 득표 계수 함수
  - 해당계층에 해당하는 표 집계
  - score(c,N) = sigma[class(y) = c]

- 유사도 반영 분류
  - 유사도의 가중치로 거리 제곱의 역수 주로 사용
  - score(c,N) = sigma[w(x,y) * [class(y)=c]]

## Nearest Neighbor Reasoning
최근접 이웃 : 가장 거리가 가까운 비슷한 객체 (Nonparametric Estimation)

- 타깃 변수를 예측하려는 데이터를 받으면 train set의 K-NN을 구함.
- 그런다음 이 k만큼의 근접이웃들을 기반해 새로운 데이터의 타깃 값 예측.
- Combining Function이 투표나 평균 같은 방식으로 타깃 값 예측.
  - 다수결 기반 분류
  - 확률 추정 (Probability) : mean을 사용하기때문에 k가 높아야 함.
    - 너무 적은 개체 수라면? 라플라스 교정법 사용.
  - 회귀 분석 : 타깃객체의 연속형 특징을 예측하고자 할때 사용. (linear regression assumption이 유사 instance사이에서는 이미 충족되므로 사용가능.)

K-NN과 K-means는 기본적으로 Skewness가 있으면 안됨(Log-transform). 때문에 outlier역시 체크.
또한 스케일이 너무 다르면 안됨(normalize). 생각해보면 당연함. Euclidean Distance를 기준으로 이웃을 선정하므로 스케일은 중요.

- k는 홀수로 할것.
- k가 5일때, 모든 5명을 똑같이 취급할수도 있지만 weightage도 넣을수 있음(weighted scoring)
  - 유사도 비중을 1/거리^2
  - 기여도를 ith 유사도비중 / 전체 유사도비중
  - 각 class에 해당하는 k들의 기여도 합으로 target data class 예측.

K 가 작으면 작을수록 당연히 Overfit문제가 발생하게됨.

##### K값 선택방법
1. 교차검증이나 내포된 예비 데이터 검사법을 사용해 여러 k에 대해 시도해보고 가장 성능이 좋은 k를 찾아낼 수 있음.
2. 최적의 k를 찾아낸 후 다시 전체 훈련 데이터를 이용해 k-NN모델을 만들면 됨.
3. 훈련데이터만을 사용하므로 별도의 시험 데이터를 통한 일반화 성능에 대해 공정하게 평가 가능.

### K-NN의 문제점
- 모델 명료성 (Intelligibility)
  - knn모델은 일반적으로 이해하기 어렵기 때문에 정당성이 중요한 경우에는 부적절.
  - 의료나 법률 분야에서 새로운 사건을 해결하려면 기존 사례 기반 학습이 적당하므로 설명이 쉽긴 함.

- 새로운 사건을 판단하는 과정에서 설명하기 쉬움
  - 왜냐면 가장 비슷한 사건을 찾아내 그것이 어케 세그먼트 되는지, 어떤값을 가질지 예측하므로 우리가 실생활에서 "에를들어~" 라고 하는 reasoning과 비슷한 맥락.

- 차원이 너무 높거나 유사도 판단과 무관한 속성이 너무 많으면 문제될수 있음.

- 계산 효율성이 떨어짐.
  - train 단계 자체는 짧음.
  - 하지만 test단계에서 이웃을 찾아야 되기때문에 여기서 느려짐.

## Clustering
비지도 학습으로 데이터에서 자연스럽게 분류되는 그룹을 찾아내는 개념.
타겟 변수에 신경쓰지 않으면서 데이터 집합에 있는 어떤 규칙성을 찾아내려는것.

- 군집화는 종종 EDA에서 사용
- 두가지 주요 군집화로 계층적 군집화(토너먼트 표같은 dendrogram)와 중점 주변 군집화(내가 알고있는 클러스터링)가 있음.

1. 계층적 군집화
- Dendrogram으로 군집을 표현.
- Dendrogram 상으로 가장 밑에 있으면서 다른 점들과 동떨어진 애들을 outlier라고 할 수 있음.
- 장점은 데이터 유사도의 지형, 즉 어떻게 그룹들로 묶을 수 있는지 알수있게 해줌.
- 원하는 군집의 개수에 따라 어디에서든 계통도를 잘라내는게 가능.

2. 중점 주변 군집화
- K-Means Clustering: k는 데이터에서 찾아내려는 군집 수
- 클러스터 별 평균계산을 통해 해당 클러스터의 중점을 구해냄.
- 그래서 각 k의 평균좌표를 centroid로 하여 계속 조정해나가면서 마지막에 그 군집을 centroid 중심으로 정의시킴.

### 군집화 결과의 이해
- 클러스터링은 EDA에서 주로 사용되는데, 보통 이름을 나열하는것만으로도 인사이트 제공가능.
- 이름을 나열해도 이해에 별 도움이 안되면 클러스터별 대표 항목 선정하는것이 도움됨.

##### K-means에서의 k는 어케결정?
- 적절한 k값을 알고싶은데 데이터에대한 이해도가 떨어질때 군집의 적합성 척도인 실루엣 스코어 사용.
- 군집들의 실루엣을 계산하기 위해서 군집간의 평균 거리 계산(dissimilarity score)
- 각 군집의 멤버들과 최근접 이웃 군집의 멤버들간의 거리를 계산
- 한 멤버의 실루엣 스코어 = (이웃군집의 멤버와의 평균거리 - 군집 내 타 멤버들과의 평균거리)/이웃군집의 멤버와의 평균 거리
- 클러스터링에대한 검증을 위해서 주로 쓰이는 방법.
- 단점으로는 군집의 수가 많은 경우 많은 계산이 필요.
