---
layout: post
title: "Data Mining Lecture"
date: 2019-05-30
image: '/assets/img/'
description: 'Lecture 12'
categories:
- Lecture Notes
- Data Mining
---

#Model Ensemble
- 수많은 모델을 만들고 조합해 만든 하나의 초우량 모델
- 일반화 성능이 향상됨.
- 더 좋은 이유는 전문가 집단과 비슷한 효과.
- k-nn기법이 앙상블 기법의 단순한 사례
- 예측정확도는 향상하지만 해석력이 떨어짐.

## 앙상블 모델의 목적
- To minimize error (Crowd of Wisdom)
- Reduce Overfitting (By combining each models' bias)
- To reduce Variance (모델간 variance를 평균같은걸로 내게되면 줄어들음.)

### Bagging vs Boosting
Bagging
- Bootstrap on train data.
- for each sample dataset, create models
- aggregate those models
- parallel processing
- Good at reducing variance and provide higher stability.
- in other words, solves over-fitting problem.
- DT,NN,NB,K-NN can be used.

Boosting
- Sequentially build weak learners
- Dataset is resampled considering weights of each observation. So more weighted sample gets sampled more for next learner.
- observation is assigned with high weight if the learner failed to learn correctly.
- Each learner is assigned with weights as importance when voting.
- reduces bias, but can increase overfitting issue.
- DT,NB can be used.

#Random Forest
- Feature Bagging 을 통해 Learner간 Correlation도 줄여줌. 결과적으로 generalization 좋아짐.
- Evaluation 은 Out-of-Bag Sample 이용. Bootstrap과정중 샘플링에 사용되지 않은 observations들을 이용하여 학습결과를 검증.
- Randomness를 극대화 하기 때문에 안정성이 높아지지만 트리가 갖는 장점인 설명력을 잃게 된다는 점은 아쉬움.
- 하이퍼파라미터에서 가장 중요한건 n_feature과 n_estimator. 깊이는 최대한 깊게 만드는게 좋음.

#Boosting
- AdaBoost uses error
- GBM uses gradient to assign weights and uses gradient descent algo to calculate optimal weight.
- XGBoost uses computational resources more efficiently
- LGBM increased performance of xgboost and minimized resource consumption. can handle large dataset. it approximates the split to increase performance.
