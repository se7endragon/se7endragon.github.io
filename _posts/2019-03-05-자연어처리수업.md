---
layout: post
title: "NLP (Deep Learning) Lecture"
date: 2019-03-05
image: '/assets/img/'
description: 'Lecture 1'
categories:
- Lecture Notes
- Deep Learning
---
# What is NLP?
- CS + AI + Linguistics
- Goal is for computers to process and understand natural language to perform some tasks

## Level of NLP
- Speech -> Phonetic analysis -> morphological analysis -> Syntactic analysis -> Semantic Interpretation -> Discourse Processing
- Text -> OCR/Tokenization -> morphological analysis -> Syntactic analysis -> Semantic Interpretation -> Discourse Processing
  - Morphological analysis - its like a parsing tree.
  - Syntactic analysis - Grammatical/ sequential
  - Semantic Interpretation - Meaning

### Some of NLP Applications
- Spell Check
- Keyword Search
- finding synonyms
- Extracting info from websites
- Classifying, reading level of school texts, pos/neg sentiment of longer doc.
- machine translation
- spoken dialog systems
- complex question answering

### NLP in Industry
- Search (written and spoken)
- Online Ads
- Automated/assisted translation
- Sentiment analysis for marketing or finance/trading
- Speech Recognition
- Automating customer support

### Why is NLP hard?
- Complexity in representing, learning, and using linguistic/situational/world/visual knowledge.
- Jane hit June and then she [fell/ran]. (who is 'she' referring to?)
- Ambiguity: "I made her duck"

### What is Deep Learning?
- DL is sub part of ML
- Most ML methods work well IF manual feature engineering was present.
- The Representation learning attempts to automatically learn good features or representations.
- DL algorithms attempt to learn multi-level representations and an output.

### Reasons for exploring DL
- Manual feature-engineered features are often over-specified, incomplete and take a long time to desing and validate.
- Learned Features are easy to adapt, fast to learn.
- DL provides very flexible, universal, learnable framework for representing world, visual and linguistic info.
- DL can learn both Supervised and Unsupervised.

#### 조교강의
- tensorflow는 파라미터 추적이 어려움.
- pytorch는 트래커기능이 있음(가중치가 어떤변화가있었는지 추적가능).
- 그래서 pytorch문법으로 오늘 실습을 진행.

## Pytorch
- gpu에 최적화가 되있음.
Python list -> numpy array -> tf/pytorch tensor.
파이토치는 텐서로 모든 자료형을 만들어서 계산을 진행함. (x = torch.empty(5,3)) -> 5x3 매트릭스(텐서) 생성.
zeros,rand 등 넘파이 function과 동일한 구조를 가지고있음.
수동으로 tensor initialize하려면

{% highlight ruby %}
torch.tensor([5.3,1.9]) #수동 tensor initialize
x.size() #np의 shape과 같음.
x.view(16) #rank 1의 텐서로 reshape.
x.view(-1,8) #row 알아서 계산하고 col을 8로 reshape.
{% endhighlight %}

- 넘파이 array와 마찬가지로 tensor 도 copy가 아니라 직접적 address access.
numpy<-> tensor

{% highlight ruby %}
x = torch.randn(4,4)
x = x.numpy() #tensor -> numpy
x = torch.from_numpy(x) #numpy -> tensor
{% endhighlight %}

CUDA TENSORS
ram말고 cuda(gpu)에 저장시킬수도있음.

{% highlight ruby %}
torch.cuda.is_available()
a =torch.from_numpy(a)
a.to(device) # to gpu
a.to('cpu',torch.double) # to cpu
{% endhighlight %}

#### Pytorch tracking
- 생성된 텐서가 NN에서 어떤 연산이 이루어져가는지를 볼 수 있는 기능.
- 이걸 통해 층계산할때 gradient를 계산할수있기 때문에 좋다.
- pytorch autograd 참조.

{% highlight ruby %}
x = torch.ones(2,2, requires_grad=True) #requires_grad를 트루로 하면 메모리에 트래킹이 시작됨.
y = x +2
z = y*y*3
out = z.mean()
print(z)
print(out)
print(x.grad) #backward가 입력되지않는상태에서는 False로 나옴.

out.backward()
print(x.grad) #gradient tensor가 나옴.
{% endhighlight %}

#### pytorch NN
- 자세한건 다큐멘테이션.
Cnn에 집어넣을 샘플 인풋 생성은

{% highlight ruby %}
sample = torch.randn(1,1,32,32) # par1 = no. sample. par2 = channels(RGB)
{% endhighlight %}

gradient is the derivative of the cost function with respect to a single weight, done for all the weights
gradient에 대해서 좀더 공부해야 겠다.
