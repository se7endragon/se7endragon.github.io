---
layout: post
title: "Data Mining Lecture"
date: 2019-03-28
image: '/assets/img/'
description: 'Lecture 4'
categories:
- Lecture Notes
- Data Mining
---

# ë°ì´í„°ì— ëŒ€í•œ ëª¨ë¸ ì í•©í™”

Parametric Modeling
- ì¼ë¶€ ìˆ˜ì¹˜í˜• íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•˜ì§€ ì•Šê³  ëª¨ë¸ êµ¬ì¡°ë§Œ ë§Œë“¤ì–´ ë†“ì€ ìƒíƒœì—ì„œ ë°ì´í„° ì„¸íŠ¸ì— fit.
- ì˜ˆë¥¼ ë“¤ì–´ ê°ì¢… regressionë“¤ì€ predefinedëœ ëª¨ì–‘ì˜ ìˆ˜ì‹ì´ ìˆê¸°ë•Œë¬¸ì— parametric modeling.
- ì¦‰ fit ì‹œí‚¬ë•Œ ìµœì ì˜ Parameterì„ ì°¾ëŠ”ê²ƒ.

Curse of Dimensionality í•´ê²°ë²•
##### Dimensionality Reduction
- missing value 90% ì´ìƒì´ë©´ remove col
- skewness 0.1ë³´ë‹¤ ì‘ìœ¼ë©´ remove col
- std 0.1ë³´ë‹¤ ì‘ìœ¼ë©´ remove col

### ì„ í˜•íŒë³„ì‹ (Linear Classifier Function)
íŒë³„ë¶„ì„ì—ì„œëŠ” observationë“¤ì´ ì†í•˜ëŠ” class labelì„ êµ¬ë³„í• ìˆ˜ìˆëŠ” decision boundaryë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒ.
- ë‹¨ìˆœíˆ ì–´ë”” classì— ì†í• ê²ƒì´ë‹¤ì—ì„œ ë©ˆì¶”ëŠ”ê²Œ ì•„ë‹ˆë¼ ëª‡% í™•ë¥ ë¡œ ì†í• ê²ƒì´ë‹¤ ë¼ëŠ”ê±¸ ì•Œ ìˆ˜ ìˆìœ¼ë©´ ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë” ë¨.
- classì— ì†í•  í™•ë¥ ì„ ì¶”ì •í• ìˆ˜ìˆëŠ” ëª¨ë¸ì´ Tree Induction for class Probability Estimationì¸ë°, í™•ë¥ ì„ ì´ìš©í•˜ì—¬ sortingì„ í•´ì„œ ì†í• í™•ë¥ ì— ëŒ€í•œ rankë¥¼ ë§¤ê¸¸ìˆ˜ìˆê³ , ì´ì—ë”°ë¥¸ ìƒìœ„ xëª…ì— ëŒ€í•œ íƒ€ê²Ÿë§ˆì¼€íŒ…ì´ ê°€ëŠ¥í•´ì§.

### Least Discriminant Analysis
íƒ€ê²Ÿ ë³€ìˆ˜ê°€ 3ê°œ ì´ìƒì˜ classification ë¬¸ì œì¼ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ .

### Logistic Regression
- Linear Classifierì„ ì‚¬ìš©í•  ê²½ìš°, ì´ê±¸ ê¸°ì¤€ìœ¼ë¡œ ë©€ë¦¬ ë–¨ì–´ì§€ë©´ ë–¨ì–´ì§ˆìˆ˜ë¡ class ì— belong í•  í™•ë¥ ì´ ë” ë†’ì•„ì§„ë‹¤.
- ê·¼ë° ë¬¸ì œëŠ” í™•ë¥ ì€ 0~1 ì‚¬ì´ê³  classifierëŠ” ë¬´í•œëŒ€ì„. ê·¸ë˜ì„œ ë²”ìœ„ì ì¸ ì°¨ì´ê°€ ë„ˆë¬´í¼.
  - Classifierë¡œë¶€í„° ê±°ë¦¬ë¥¼ í™•ë¥ ë¡œ ë³€í™˜. (0-1)
  - ì‚¬ê±´ì´ ì¼ì–´ë‚  ê°€ëŠ¥ì„±ì„ í‘œí˜„í•˜ëŠ” ìŠ¹ì‚°(Odds) êµ¬í•˜ê¸°.
  - ì´ë‘ê°€ì§€ë¥¼ ì´ìš©í•˜ì—¬ Log-odds êµ¬í•¨.
  - ê·¸ë˜ì„œ Logistic Regressionì˜ f(x)ëŠ” íŠ¹ì • classì— ì†í•  log-Odds êµ¬í•˜ëŠ” ê²ƒì´ ë¨.
  - ì´í›„ ì´ ë¡œê·¸ ìŠ¹ì‚°ì„ ì†í•  í™•ë¥ ë¡œ ë‹¤ì‹œ ë³€í™˜ ê°€ëŠ¥.

0.8 ì´ìƒ correlationì€ ì¤‘ë³µì œê±°
ëŒ€ì‹  contextì— ëŒ€í•œ ì´í•´ê°€ ë¨¼ì € í•„ìš”í•œ ìƒíƒœì—ì„œ ê²°ì •í•˜ëŠ”ê²Œ ì¤‘ìš”.
drop the variable which is less correlated with the response
drop the variable which has larger Variance Inflation Factor (VIF)
fit 2 new models : one removing A and one removing B, and to see which new model suffers less multicollinearity. To measure multicollinearity, you can look at the VIF of each present variable or the Condition Number of ğ‘‹â€²ğ‘‹ where ğ‘‹ is the present design matrix
use variable selection method like stepwise regression and lasso regression to (hopefully) remove one variable
